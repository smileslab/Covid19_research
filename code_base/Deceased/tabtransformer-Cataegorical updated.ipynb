{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzgx4WDUyVFG"
      },
      "source": [
        "# Structured data learning with TabTransformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "UI83-ZLhw5wt"
      },
      "outputs": [],
      "source": [
        "# pip install -U tensorflow-addons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i3nzfdvttE3u",
        "outputId": "79ae2952-fa05-4e6d-f727-b2a9bc0f6d2f"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "lRHLWDaMyVFN"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-12-15 20:17:46.269095: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /cm/local/apps/gcc/10.2.0/lib:/cm/local/apps/gcc/10.2.0/lib64:/cm/local/apps/gcc/10.2.0/lib32:/cm/shared/apps/slurm/current/lib64/slurm:/cm/shared/apps/slurm/current/lib64\n",
            "2022-12-15 20:17:46.269117: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
            "/home/o/oananbeh/.conda/envs/notebook-env/lib/python3.9/site-packages/tensorflow_addons/utils/ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.8.0 and strictly below 2.11.0 (nightly versions are not supported). \n",
            " The versions of TensorFlow you are currently using is 2.7.0 and is not supported. \n",
            "Some things might work, some things might not.\n",
            "If you were to encounter a bug, do not file an issue.\n",
            "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
            "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
            "https://github.com/tensorflow/addons\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow_addons as tfa\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "InxAlIS8yVFQ"
      },
      "source": [
        "## Prepare the data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "SWwY76FsszRb"
      },
      "outputs": [],
      "source": [
        "NUMERIC_FEATURE_NAMES=['AGE', 'PEFF_Temperature', 'PEFF_Respiratory_Rate', 'PEFF_Pulse',\n",
        "       'PEFF_BP_Systolic', 'PEFF_BP_Diastolic', 'PEFF_Glasgow',\n",
        "       'ABGF_Ph', 'ABGF_Pa_O2', 'ABGF_Pa_CO2','ABGF_HCO3',\n",
        "       'WBC', 'PNN', 'Lymphocytes', 'Hemoglobin', 'Platelets',\n",
        "       'Creatinine', 'ALT', 'LDH', 'FERRITIN', 'D_DIMER', 'CRP',\n",
        "       'PROCALCITONI', 'TROPONIN', 'Pro_BNP', 'PTT', 'Vitamin_D', 'IL6',\n",
        "       ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "28"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(NUMERIC_FEATURE_NAMES)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "CATEGORICAL_FEATURES=['PatientId','G.Nationality','Gender','C_Diabetes', 'C_HTN',\n",
        "       'C.Heart ischemic', 'C.Heart failure', 'C.Cardiomyopathies',\n",
        "       'C.End stage renal', 'C.Hemodialysis', 'C.COPD',\n",
        "       'C.Lung Interstitial Disease ', 'C.Bronchial Asthma',\n",
        "       'C.Cerebrovascular', 'C.Neurologic (dementia)',\n",
        "       'C.History of psychiatric', 'C.Cirrhosis', 'C.liver disease',\n",
        "       'C.Obesity', 'C.Sick cell', 'C.Cancer', 'C.Solid organ transplant',\n",
        "       'IS.Hematopoietic cell transplant', 'IS.HIV', 'IS.corticosteroids',\n",
        "       'IS.Other immunosuppressing agents', 'IS.Other immunodeficiencies',\n",
        "       'IS.Pregnancy', 'IS.Smoker',  'CSA_Fever', 'CSA_SOB', 'CSA.Chest pain', 'CSA.Confusion',\n",
        "       'CSA.Hemoptysis', 'CSA.Diarrhea', 'CSA_Cough', 'CSA.Myalgia',\n",
        "       'CSA.Headache', 'CSA.Abdominal Pain', 'CSA.Nausea or Vomiting',\n",
        "       'CSA.Loss of Smell or Tast', 'PO.Condition', 'PO.Current Condition',\n",
        "       'PEFF.Nasal Cannula', 'PEFF.Mask', 'PEFF.HFNC',\n",
        "       'PEFF.If patient need prone position', 'PEFF.If patient intubated',\n",
        "       'PEFF.If Patient required Psychiatric Consultation',\n",
        "       'PEFF.Presence of thrombo-embolic', 'PEFF.Confirmed DVT',\n",
        "       'PEFF.Confirmed Pulmonary embolism',\n",
        "       'PEFF.Confirmed Myocardial infarction', 'PEFF.Confirmed CVA/TIA',\n",
        "       'MPA_Antibiotics','Anticoagulant','Immunomodulators','antiviral',\n",
        "       'XrayResult']  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "59"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(CATEGORICAL_FEATURES)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# fg=(NUMERIC_FEATURE_NAMES+CATEGORICAL_FEATURES)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "iGvuDdmSr-Co"
      },
      "outputs": [],
      "source": [
        "path='/home/o/oananbeh/notebook/Experment2/src/TabTransformer-multiclass_died/'\n",
        "dataSetPath=path+'SMOT_died_DataSet.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "1aczTZrSru-w"
      },
      "outputs": [],
      "source": [
        "data=pd.read_csv(dataSetPath)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(76, 92)"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "# dd=c+list(data.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "# newdata=pd.DataFrame()\n",
        "# data['AGE']=data['AGE'].astype('int64')\n",
        "# newdata['AGE']=data['AGE']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "# for index, c in newdata.iterrows():\n",
        "#     if (c['AGE']>=56 and c['AGE'] <=73):\n",
        "#         data[['AGE']]=data[['AGE']].replace(c['AGE'], 'Baby Boomers')  \n",
        "#     if (c['AGE']>=38 and c['AGE'] <=55):\n",
        "#         data[['AGE']]=data[['AGE']].replace(c['AGE'], 'Generation-X')  \n",
        "#     if (c['AGE'] <=37):\n",
        "#         data[['AGE']]=data[['AGE']].replace(c['AGE'], 'Generation-Y')  \n",
        "#     if (c['AGE']>=74):\n",
        "#         data[['AGE']]=data[['AGE']].replace(c['AGE'], 'Silent Generation')  \n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "for feature in CATEGORICAL_FEATURES[2:]:\n",
        "    data[feature] = data[feature].map({1:'yes', 0:'no'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "data[CATEGORICAL_FEATURES]=data[CATEGORICAL_FEATURES].astype('object')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Nxdxd4VJkU_n"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(76, 92)"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "v-iYxvn5ra9q"
      },
      "outputs": [],
      "source": [
        "CSV_HEADER=list(data.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataSetPath=path+'SMOT_died_DataSet2.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "data.to_csv(dataSetPath, index=False, header=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "final_dataSet = data.drop('OUTCOME', axis=1)\n",
        "target = data.OUTCOME"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run ML On SMOTE Dataset\n",
        "#Create final train and test sets startified according to the target variable. \n",
        "X_train, X_test, y_train, y_test = train_test_split(final_dataSet,target, test_size = 0.30,stratify=target, \n",
        "                                                    random_state=123)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_data=pd.concat([X_train, y_train],axis=1)\n",
        "test_data=pd.concat([X_test, y_test],axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "oae-v6X-yVFR"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train dataset shape: (53, 92)\n",
            "Test dataset shape: (23, 92)\n"
          ]
        }
      ],
      "source": [
        "train_data = pd.read_csv(dataSetPath, header=None, names=CSV_HEADER)\n",
        "test_data = pd.read_csv(dataSetPath, header=None, names=CSV_HEADER)\n",
        "train_data=train_data.sample(frac = 0.7)\n",
        "test_data=test_data.sample(frac = 0.3)\n",
        "print(f\"Train dataset shape: {train_data.shape}\")\n",
        "print(f\"Test dataset shape: {test_data.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train data class counts: Counter({'died=>3': 28, 'died=<3': 25})\n"
          ]
        }
      ],
      "source": [
        "target = train_data.OUTCOME\n",
        "print(f\"train data class counts: {Counter(target)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "test data class counts: Counter({'died=<3': 12, 'died=>3': 11})\n"
          ]
        }
      ],
      "source": [
        "target = test_data.OUTCOME\n",
        "print(f\"test data class counts: {Counter(target)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTAZgXOjyVFT"
      },
      "source": [
        "*Now* we store the training and test data in separate CSV files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "XYkn8VWkyVFU"
      },
      "outputs": [],
      "source": [
        "train_data_file = path+\"train_data.csv\"\n",
        "test_data_file = path+\"test_data.csv\"\n",
        "\n",
        "train_data.to_csv(train_data_file, index=False, header=False)\n",
        "test_data.to_csv(test_data_file, index=False, header=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4LZWAoUyVFV"
      },
      "source": [
        "## Define dataset metadata\n",
        "\n",
        "Here, we define the metadata of the dataset that will be useful for reading and parsing\n",
        "the data into input features, and encoding the input features with respect to their types."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "oZr5RNk0yVFV"
      },
      "outputs": [],
      "source": [
        "\n",
        "# A dictionary of the categorical features and their vocabulary.\n",
        "CATEGORICAL_FEATURES_WITH_VOCABULARY={}\n",
        "for featrue in CATEGORICAL_FEATURES:\n",
        "    CATEGORICAL_FEATURES_WITH_VOCABULARY[featrue]=sorted(list(data[featrue].unique()))\n",
        "\n",
        "# A list of the categorical feature names.\n",
        "CATEGORICAL_FEATURE_NAMES = list(CATEGORICAL_FEATURES_WITH_VOCABULARY.keys())\n",
        "# A list of all the input features.\n",
        "FEATURE_NAMES = NUMERIC_FEATURE_NAMES + CATEGORICAL_FEATURE_NAMES\n",
        "# A list of column default values for each feature.\n",
        "COLUMN_DEFAULTS = [\n",
        "    [0.0] if feature_name in NUMERIC_FEATURE_NAMES  else [\"NA\"]\n",
        "    for feature_name in CSV_HEADER\n",
        "]\n",
        "\n",
        "# The name of the target feature.\n",
        "TARGET_FEATURE_NAME = 'OUTCOME'\n",
        "# A list of the labels of the target features.\n",
        "# TARGET_LABELS = [' discharge=<2',' discharge=<4','died=<3',' discharge>4','died=>3','discharge=<1',' discharge=<3']\n",
        "TARGET_LABELS = ['died=<3', 'died=>3']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwREvZFwyVFW"
      },
      "source": [
        "## Configure the hyperparameters\n",
        "\n",
        "The hyperparameters includes model architecture and training configurations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "vjWETewYyVFX"
      },
      "outputs": [],
      "source": [
        "LEARNING_RATE = 0.001\n",
        "WEIGHT_DECAY = 0.0001\n",
        "DROPOUT_RATE = 0.2\n",
        "BATCH_SIZE = 8\n",
        "NUM_EPOCHS = 15\n",
        "\n",
        "NUM_TRANSFORMER_BLOCKS = 3  # Number of transformer blocks.\n",
        "NUM_HEADS = 4  # Number of attention heads.\n",
        "EMBEDDING_DIMS = 16  # Embedding dimensions of the categorical features.\n",
        "MLP_HIDDEN_UNITS_FACTORS = [\n",
        "    2,\n",
        "    1,\n",
        "]  # MLP hidden layer units, as factors of the number of inputs.\n",
        "NUM_MLP_BLOCKS = 2  # Number of MLP blocks in the baseline model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Djjh3lu9yVFd"
      },
      "source": [
        "## Implement data reading pipeline\n",
        "\n",
        "We define an input function that reads and parses the file, then converts features\n",
        "and labels into a[`tf.data.Dataset`](https://www.tensorflow.org/guide/datasets)\n",
        "for training or evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "M7nFPas1yVFd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-12-15 20:17:58.871389: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /cm/local/apps/gcc/10.2.0/lib:/cm/local/apps/gcc/10.2.0/lib64:/cm/local/apps/gcc/10.2.0/lib32:/cm/shared/apps/slurm/current/lib64/slurm:/cm/shared/apps/slurm/current/lib64\n",
            "2022-12-15 20:17:58.871506: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /cm/local/apps/gcc/10.2.0/lib:/cm/local/apps/gcc/10.2.0/lib64:/cm/local/apps/gcc/10.2.0/lib32:/cm/shared/apps/slurm/current/lib64/slurm:/cm/shared/apps/slurm/current/lib64\n",
            "2022-12-15 20:17:58.871558: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /cm/local/apps/gcc/10.2.0/lib:/cm/local/apps/gcc/10.2.0/lib64:/cm/local/apps/gcc/10.2.0/lib32:/cm/shared/apps/slurm/current/lib64/slurm:/cm/shared/apps/slurm/current/lib64\n",
            "2022-12-15 20:17:58.871609: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /cm/local/apps/gcc/10.2.0/lib:/cm/local/apps/gcc/10.2.0/lib64:/cm/local/apps/gcc/10.2.0/lib32:/cm/shared/apps/slurm/current/lib64/slurm:/cm/shared/apps/slurm/current/lib64\n",
            "2022-12-15 20:17:58.871666: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /cm/local/apps/gcc/10.2.0/lib:/cm/local/apps/gcc/10.2.0/lib64:/cm/local/apps/gcc/10.2.0/lib32:/cm/shared/apps/slurm/current/lib64/slurm:/cm/shared/apps/slurm/current/lib64\n",
            "2022-12-15 20:17:58.871718: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /cm/local/apps/gcc/10.2.0/lib:/cm/local/apps/gcc/10.2.0/lib64:/cm/local/apps/gcc/10.2.0/lib32:/cm/shared/apps/slurm/current/lib64/slurm:/cm/shared/apps/slurm/current/lib64\n",
            "2022-12-15 20:17:58.871770: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /cm/local/apps/gcc/10.2.0/lib:/cm/local/apps/gcc/10.2.0/lib64:/cm/local/apps/gcc/10.2.0/lib32:/cm/shared/apps/slurm/current/lib64/slurm:/cm/shared/apps/slurm/current/lib64\n",
            "2022-12-15 20:17:58.871822: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /cm/local/apps/gcc/10.2.0/lib:/cm/local/apps/gcc/10.2.0/lib64:/cm/local/apps/gcc/10.2.0/lib32:/cm/shared/apps/slurm/current/lib64/slurm:/cm/shared/apps/slurm/current/lib64\n",
            "2022-12-15 20:17:58.871832: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
            "Skipping registering GPU devices...\n",
            "2022-12-15 20:17:58.872234: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        }
      ],
      "source": [
        "target_label_lookup = layers.StringLookup(\n",
        "    vocabulary=TARGET_LABELS, mask_token=None, num_oov_indices=0\n",
        ")\n",
        "\n",
        "\n",
        "def prepare_example(features, target):\n",
        "    # print(target)\n",
        "    target_index = target_label_lookup(target)\n",
        "    # weights = features.pop(WEIGHT_COLUMN_NAME), weights\n",
        "    return features, target_index \n",
        "\n",
        "# func_labels = lambda x, y: x, np.array(targets[y])\n",
        "\n",
        "\n",
        "def get_dataset_from_csv(csv_file_path, batch_size=128, shuffle=False):\n",
        "    dataset = tf.data.experimental.make_csv_dataset(\n",
        "        csv_file_path,\n",
        "        batch_size=batch_size,\n",
        "        column_names=CSV_HEADER,\n",
        "        column_defaults=COLUMN_DEFAULTS,\n",
        "        label_name=TARGET_FEATURE_NAME,\n",
        "        num_epochs=1,\n",
        "        header=False,\n",
        "        na_value=\"?\",\n",
        "        shuffle=shuffle,\n",
        "    ).map(prepare_example, num_parallel_calls=tf.data.AUTOTUNE, deterministic=False) \n",
        "    # dataset.take(1)\n",
        "    \n",
        "    # df = pd.read_csv(csv_file_path,  names=CSV_HEADER)\n",
        "    # # print(df.columns)\n",
        "    # features = df.drop('OUTCOME', axis=1)\n",
        "    # labels = df.pop('OUTCOME')\n",
        "    # # print(labels.unique())\n",
        "    # labels = labels.map(targets)\n",
        "    # labels\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "from keras import backend as K\n",
        "def get_f1(y_true, y_pred): #taken from old keras source code\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
        "    return f1_val\n",
        "def recall(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall_keras = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall_keras\n",
        "def precision(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision_keras = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision_keras \n",
        "def negative_predictive_value(y_true, y_pred):\n",
        "    tn = K.sum(K.round(K.clip((1 - y_true) * (1 - y_pred), 0, 1)))\n",
        "    fn = K.sum(K.round(K.clip(y_true * (1 - y_pred), 0, 1)))\n",
        "    return tn / (tn + fn + K.epsilon())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CategoricalTruePositives(keras.metrics.Metric):\n",
        "    def __init__(self, name=\"categorical_true_positives\", **kwargs):\n",
        "        super(CategoricalTruePositives, self).__init__(name=name, **kwargs)\n",
        "        self.true_positives = self.add_weight(name=\"ctp\", initializer=\"zeros\")\n",
        "\n",
        "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "        y_pred = tf.reshape(tf.argmax(y_pred, axis=1), shape=(-1, 1))\n",
        "        values = tf.cast(y_true, \"int32\") == tf.cast(y_pred, \"int32\")\n",
        "        values = tf.cast(values, \"float32\")\n",
        "        if sample_weight is not None:\n",
        "            sample_weight = tf.cast(sample_weight, \"float32\")\n",
        "            values = tf.multiply(values, sample_weight)\n",
        "        self.true_positives.assign_add(tf.reduce_sum(values))\n",
        "\n",
        "    def result(self):\n",
        "        return self.true_positives\n",
        "\n",
        "    def reset_state(self):\n",
        "        # The state of the metric will be reset at the start of each epoch.\n",
        "        self.true_positives.assign(0.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOkSA0StyVFe"
      },
      "source": [
        "## Implement a training and evaluation procedure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "VZ0uo4xiyVFe"
      },
      "outputs": [],
      "source": [
        "\n",
        "def run_experiment(\n",
        "    model,\n",
        "    train_data_file,\n",
        "    test_data_file,\n",
        "    num_epochs,\n",
        "    learning_rate,\n",
        "    weight_decay,\n",
        "    batch_size,\n",
        "):\n",
        "\n",
        "    optimizer = tfa.optimizers.AdamW(\n",
        "        learning_rate=learning_rate, weight_decay=weight_decay\n",
        "    )\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss=keras.losses.SparseCategoricalCrossentropy(),#change this \n",
        "        metrics=[keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),CategoricalTruePositives(),get_f1,precision,recall], #change this one \n",
        "    )\n",
        "    #keras.metrics.AUC(name='AUC_ROC'),tf.keras.metrics.Precision(),tf.keras.metrics.Recall()\n",
        "    train_dataset = get_dataset_from_csv(train_data_file, batch_size, shuffle=True)\n",
        "    validation_dataset = get_dataset_from_csv(test_data_file, batch_size)\n",
        "    # df_train=pd.read_csv(train_data_file)\n",
        "    # df_test=pd.read_csv(test_data_file)\n",
        "\n",
        "    # ohc_df_train = pd.get_dummies(df_train)\n",
        "    # ohc_df_test = pd.get_dummies(df_test)\n",
        "    # train_dataset = tf.keras.utils.to_categorical(ohc_df_train, num_classes=7)\n",
        "    # # validation_dataset = tf.keras.utils.to_categorical(ohc_df_test, num_classes=7)\n",
        "    # print(train_features.shape)\n",
        "    # print(train_labels.shape)\n",
        "    print(\"Start training the model...\")\n",
        "    history = model.fit(\n",
        "        train_dataset, epochs=num_epochs, validation_data=validation_dataset\n",
        "    )\n",
        "    print(\"Model training finished\")\n",
        "\n",
        "    _, accuracy,tp,f1,prec,rec= model.evaluate(validation_dataset, verbose=0)\n",
        "    \n",
        "    # print(f\"Validation f1: {round(f1 * 100, 2)}%\")\n",
        "    # print(f\"Validation accuracy: {round(accuracy * 100, 2)}%\")\n",
        "    # print(f\"Validation Precision: {round(prec * 100, 2)}%\")\n",
        "    # print(f\"Validation Recall: {round(rec * 100, 2)}%\")\n",
        "    print(f\"Validation f1: {round(f1,6)}\")\n",
        "    print(f\"Validation accuracy: {round(accuracy,6)}\")\n",
        "    print(f\"Validation Precision: {round(prec,6 )}\")\n",
        "    print(f\"Validation Recall: {round(rec,6)}\")\n",
        "    # print(f\"categorical true positives: {round(tp)}\")\n",
        "    y_pred = model.predict(validation_dataset)\n",
        "    return history\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-vB_tU_HyVFf"
      },
      "source": [
        "## Create model inputs\n",
        "\n",
        "Now, define the inputs for the models as a dictionary, where the key is the feature name,\n",
        "and the value is a `keras.layers.Input` tensor with the corresponding feature shape\n",
        "and data type."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "4P3AofFXyVFf"
      },
      "outputs": [],
      "source": [
        "\n",
        "def create_model_inputs():\n",
        "    inputs = {}\n",
        "    for feature_name in FEATURE_NAMES:\n",
        "        if feature_name in NUMERIC_FEATURE_NAMES:\n",
        "            \n",
        "            inputs[feature_name] = layers.Input(\n",
        "                name=feature_name, shape=(), dtype=tf.float32\n",
        "            )\n",
        "        else:\n",
        "            print(feature_name)\n",
        "            inputs[feature_name] = layers.Input(\n",
        "                name=feature_name, shape=(), dtype=tf.string\n",
        "            )\n",
        "    return inputs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ylh_XJYfyVFg"
      },
      "source": [
        "## Encode features\n",
        "\n",
        "The `encode_inputs` method returns `encoded_categorical_feature_list` and `numerical_feature_list`.\n",
        "We encode the categorical features as embeddings, using a fixed `embedding_dims` for all the features,\n",
        "regardless their vocabulary sizes. This is required for the Transformer model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "dJGDE73gyVFg"
      },
      "outputs": [],
      "source": [
        "\n",
        "def encode_inputs(inputs, embedding_dims):\n",
        "\n",
        "    encoded_categorical_feature_list = []\n",
        "    numerical_feature_list = []\n",
        "\n",
        "    for feature_name in inputs:\n",
        "        if feature_name in CATEGORICAL_FEATURE_NAMES:\n",
        "\n",
        "            # Get the vocabulary of the categorical feature.\n",
        "            vocabulary = CATEGORICAL_FEATURES_WITH_VOCABULARY[feature_name]\n",
        "\n",
        "            # Create a lookup to convert string values to an integer indices.\n",
        "            # Since we are not using a mask token nor expecting any out of vocabulary\n",
        "            # (oov) token, we set mask_token to None and  num_oov_indices to 0.\n",
        "            lookup = layers.StringLookup(\n",
        "                vocabulary=vocabulary,\n",
        "                mask_token=None,\n",
        "                num_oov_indices=0,\n",
        "                output_mode=\"int\",\n",
        "            )\n",
        "\n",
        "            # Convert the string input values into integer indices.\n",
        "            encoded_feature = lookup(inputs[feature_name])\n",
        "\n",
        "            # Create an embedding layer with the specified dimensions.\n",
        "            embedding = layers.Embedding(\n",
        "                input_dim=len(vocabulary), output_dim=embedding_dims\n",
        "            )\n",
        "\n",
        "            # Convert the index values to embedding representations.\n",
        "            encoded_categorical_feature = embedding(encoded_feature)\n",
        "            encoded_categorical_feature_list.append(encoded_categorical_feature)\n",
        "\n",
        "        else:\n",
        "\n",
        "            # Use the numerical features as-is.\n",
        "            numerical_feature = tf.expand_dims(inputs[feature_name], -1)\n",
        "            numerical_feature_list.append(numerical_feature)\n",
        "\n",
        "    return encoded_categorical_feature_list, numerical_feature_list\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8rwyx5uyVFg"
      },
      "source": [
        "## Implement an MLP block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "JBQIVJvFyVFh"
      },
      "outputs": [],
      "source": [
        "\n",
        "def create_mlp(hidden_units, dropout_rate, activation, normalization_layer, name=None):\n",
        "\n",
        "    mlp_layers = []\n",
        "    for units in hidden_units:\n",
        "        mlp_layers.append(normalization_layer),\n",
        "        mlp_layers.append(layers.Dense(units, activation=activation))\n",
        "        mlp_layers.append(layers.Dropout(dropout_rate))\n",
        "\n",
        "    return keras.Sequential(mlp_layers, name=name)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0eWonKUyVFi"
      },
      "source": [
        "## Experiment 2: TabTransformer\n",
        "\n",
        "The TabTransformer architecture works as follows:\n",
        "\n",
        "1. All the categorical features are encoded as embeddings, using the same `embedding_dims`.\n",
        "This means that each value in each categorical feature will have its own embedding vector.\n",
        "2. A column embedding, one embedding vector for each categorical feature, is added (point-wise) to the categorical feature embedding.\n",
        "3. The embedded categorical features are fed into a stack of Transformer blocks.\n",
        "Each Transformer block consists of a multi-head self-attention layer followed by a feed-forward layer.\n",
        "3. The outputs of the final Transformer layer, which are the *contextual embeddings* of the categorical features,\n",
        "are concatenated with the input numerical features, and fed into a final MLP block.\n",
        "4. A `softmax` classifer is applied at the end of the model.\n",
        "\n",
        "The [paper](https://arxiv.org/abs/2012.06678) discusses both addition and concatenation of the column embedding in the\n",
        "*Appendix: Experiment and Model Details* section.\n",
        "The architecture of TabTransformer is shown below, as presented in the paper.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/keras-team/keras-io/master/examples/structured_data/img/tabtransformer/tabtransformer.png\" width=\"500\"/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "E_oC25OzyVFj",
        "outputId": "14735b31-c514-47a5-865f-4f9aecb310a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PatientId\n",
            "G.Nationality\n",
            "Gender\n",
            "C_Diabetes\n",
            "C_HTN\n",
            "C.Heart ischemic\n",
            "C.Heart failure\n",
            "C.Cardiomyopathies\n",
            "C.End stage renal\n",
            "C.Hemodialysis\n",
            "C.COPD\n",
            "C.Lung Interstitial Disease \n",
            "C.Bronchial Asthma\n",
            "C.Cerebrovascular\n",
            "C.Neurologic (dementia)\n",
            "C.History of psychiatric\n",
            "C.Cirrhosis\n",
            "C.liver disease\n",
            "C.Obesity\n",
            "C.Sick cell\n",
            "C.Cancer\n",
            "C.Solid organ transplant\n",
            "IS.Hematopoietic cell transplant\n",
            "IS.HIV\n",
            "IS.corticosteroids\n",
            "IS.Other immunosuppressing agents\n",
            "IS.Other immunodeficiencies\n",
            "IS.Pregnancy\n",
            "IS.Smoker\n",
            "CSA_Fever\n",
            "CSA_SOB\n",
            "CSA.Chest pain\n",
            "CSA.Confusion\n",
            "CSA.Hemoptysis\n",
            "CSA.Diarrhea\n",
            "CSA_Cough\n",
            "CSA.Myalgia\n",
            "CSA.Headache\n",
            "CSA.Abdominal Pain\n",
            "CSA.Nausea or Vomiting\n",
            "CSA.Loss of Smell or Tast\n",
            "PO.Condition\n",
            "PO.Current Condition\n",
            "PEFF.Nasal Cannula\n",
            "PEFF.Mask\n",
            "PEFF.HFNC\n",
            "PEFF.If patient need prone position\n",
            "PEFF.If patient intubated\n",
            "PEFF.If Patient required Psychiatric Consultation\n",
            "PEFF.Presence of thrombo-embolic\n",
            "PEFF.Confirmed DVT\n",
            "PEFF.Confirmed Pulmonary embolism\n",
            "PEFF.Confirmed Myocardial infarction\n",
            "PEFF.Confirmed CVA/TIA\n",
            "MPA_Antibiotics\n",
            "Anticoagulant\n",
            "Immunomodulators\n",
            "antiviral\n",
            "XrayResult\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "Total model weights: 3804550\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/o/oananbeh/.conda/envs/notebook-env/lib/python3.9/site-packages/keras/engine/functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  layer_config = serialize_layer_fn(layer)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def create_tabtransformer_classifier(\n",
        "    num_transformer_blocks,\n",
        "    num_heads,\n",
        "    embedding_dims,\n",
        "    mlp_hidden_units_factors,\n",
        "    dropout_rate,\n",
        "    use_column_embedding=False,\n",
        "):\n",
        "\n",
        "    # Create model inputs.\n",
        "    inputs = create_model_inputs()\n",
        "    # encode features.\n",
        "    encoded_categorical_feature_list, numerical_feature_list = encode_inputs(\n",
        "        inputs, embedding_dims\n",
        "    )\n",
        "    # Stack categorical feature embeddings for the Tansformer.\n",
        "    encoded_categorical_features = tf.stack(encoded_categorical_feature_list, axis=1)\n",
        "    # Concatenate numerical features.\n",
        "    numerical_features = layers.concatenate(numerical_feature_list)\n",
        "\n",
        "    # Add column embedding to categorical feature embeddings.\n",
        "    if use_column_embedding:\n",
        "        num_columns = encoded_categorical_features.shape[1]\n",
        "        column_embedding = layers.Embedding(\n",
        "            input_dim=num_columns, output_dim=embedding_dims\n",
        "        )\n",
        "        column_indices = tf.range(start=0, limit=num_columns, delta=1)\n",
        "        encoded_categorical_features = encoded_categorical_features + column_embedding(\n",
        "            column_indices\n",
        "        )\n",
        "\n",
        "    # Create multiple layers of the Transformer block.\n",
        "    for block_idx in range(num_transformer_blocks):\n",
        "        # Create a multi-head attention layer.\n",
        "        attention_output = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads,\n",
        "            key_dim=embedding_dims,\n",
        "            dropout=dropout_rate,\n",
        "            name=f\"multihead_attention_{block_idx}\",\n",
        "        )(encoded_categorical_features, encoded_categorical_features)\n",
        "        # Skip connection 1.\n",
        "        x = layers.Add(name=f\"skip_connection1_{block_idx}\")(\n",
        "            [attention_output, encoded_categorical_features]\n",
        "        )\n",
        "        # Layer normalization 1.\n",
        "        x = layers.LayerNormalization(name=f\"layer_norm1_{block_idx}\", epsilon=1e-6)(x)\n",
        "        # Feedforward.\n",
        "        feedforward_output = create_mlp(\n",
        "            hidden_units=[embedding_dims],\n",
        "            dropout_rate=dropout_rate,\n",
        "            activation=keras.activations.gelu,\n",
        "            normalization_layer=layers.LayerNormalization(epsilon=1e-6),\n",
        "            name=f\"feedforward_{block_idx}\",\n",
        "        )(x)\n",
        "        # Skip connection 2.\n",
        "        x = layers.Add(name=f\"skip_connection2_{block_idx}\")([feedforward_output, x])\n",
        "        # Layer normalization 2.\n",
        "        encoded_categorical_features = layers.LayerNormalization(\n",
        "            name=f\"layer_norm2_{block_idx}\", epsilon=1e-6\n",
        "        )(x)\n",
        "\n",
        "    # Flatten the \"contextualized\" embeddings of the categorical features.\n",
        "    categorical_features = layers.Flatten()(encoded_categorical_features)\n",
        "    # Apply layer normalization to the numerical features.\n",
        "    numerical_features = layers.LayerNormalization(epsilon=1e-6)(numerical_features)\n",
        "    # Prepare the input for the final MLP block.\n",
        "    features = layers.concatenate([categorical_features, numerical_features])\n",
        "\n",
        "    # Compute MLP hidden_units.\n",
        "    mlp_hidden_units = [\n",
        "        factor * features.shape[-1] for factor in mlp_hidden_units_factors\n",
        "    ]\n",
        "    # Create final MLP.\n",
        "    features = create_mlp(\n",
        "        hidden_units=mlp_hidden_units,\n",
        "        dropout_rate=dropout_rate,\n",
        "        activation=keras.activations.selu,\n",
        "        normalization_layer=layers.BatchNormalization(),\n",
        "        name=\"MLP\",\n",
        "    )(features)\n",
        "\n",
        "    # Add a sigmoid as a binary classifer.\n",
        "    outputs = layers.Dense(units=2, activation=\"softmax\", name=\"softmax\")(features) #change this \n",
        "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "    model.save(path+'my_model.h5')\n",
        "    return model\n",
        "\n",
        "\n",
        "tabtransformer_model = create_tabtransformer_classifier(\n",
        "    num_transformer_blocks=NUM_TRANSFORMER_BLOCKS,\n",
        "    num_heads=NUM_HEADS,\n",
        "    embedding_dims=EMBEDDING_DIMS,\n",
        "    mlp_hidden_units_factors=MLP_HIDDEN_UNITS_FACTORS,\n",
        "    dropout_rate=DROPOUT_RATE,\n",
        ")\n",
        "\n",
        "print(\"Total model weights:\", tabtransformer_model.count_params())\n",
        "# keras.utils.plot_model(tabtransformer_model, show_shapes=True, rankdir=\"LR\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VkT6ZdxIyVFj"
      },
      "source": [
        "Let's train and evaluate the TabTransformer model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "uFaH32AOyVFk"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start training the model...\n",
            "Epoch 1/15\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/o/oananbeh/.conda/envs/notebook-env/lib/python3.9/site-packages/keras/engine/functional.py:559: UserWarning: Input dict contained keys ['INR', 'AST', 'CPK', 'Fibrinogen'] which did not match any model input. They will be ignored by the model.\n",
            "  inputs = self._flatten_to_reference_inputs(inputs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "7/7 [==============================] - 10s 383ms/step - loss: 3.2655 - accuracy: 0.4906 - categorical_true_positives: 26.0000 - get_f1: 0.6609 - precision: 0.5107 - recall: 1.0000 - val_loss: 2.5515 - val_accuracy: 0.6087 - val_categorical_true_positives: 14.0000 - val_get_f1: 0.6190 - val_precision: 0.4762 - val_recall: 1.0000\n",
            "Epoch 2/15\n",
            "7/7 [==============================] - 1s 86ms/step - loss: 5.1496 - accuracy: 0.7358 - categorical_true_positives: 39.0000 - get_f1: 0.6906 - precision: 0.5321 - recall: 1.0000 - val_loss: 9.6531 - val_accuracy: 0.5217 - val_categorical_true_positives: 12.0000 - val_get_f1: 0.6190 - val_precision: 0.4762 - val_recall: 1.0000\n",
            "Epoch 3/15\n",
            "7/7 [==============================] - 1s 88ms/step - loss: 6.4041 - accuracy: 0.7547 - categorical_true_positives: 40.0000 - get_f1: 0.6879 - precision: 0.5321 - recall: 1.0000 - val_loss: 23.7719 - val_accuracy: 0.4783 - val_categorical_true_positives: 11.0000 - val_get_f1: 0.6190 - val_precision: 0.4762 - val_recall: 1.0000\n",
            "Epoch 4/15\n",
            "7/7 [==============================] - 1s 87ms/step - loss: 5.7967 - accuracy: 0.6981 - categorical_true_positives: 37.0000 - get_f1: 0.6582 - precision: 0.5107 - recall: 1.0000 - val_loss: 15.0647 - val_accuracy: 0.4783 - val_categorical_true_positives: 11.0000 - val_get_f1: 0.6190 - val_precision: 0.4762 - val_recall: 1.0000\n",
            "Epoch 5/15\n",
            "7/7 [==============================] - 1s 86ms/step - loss: 5.2014 - accuracy: 0.6038 - categorical_true_positives: 32.0000 - get_f1: 0.6749 - precision: 0.5214 - recall: 1.0000 - val_loss: 2.1681 - val_accuracy: 0.6957 - val_categorical_true_positives: 16.0000 - val_get_f1: 0.6190 - val_precision: 0.4762 - val_recall: 1.0000\n",
            "Epoch 6/15\n",
            "7/7 [==============================] - 1s 88ms/step - loss: 6.2611 - accuracy: 0.6792 - categorical_true_positives: 36.0000 - get_f1: 0.6650 - precision: 0.5214 - recall: 1.0000 - val_loss: 1.6348 - val_accuracy: 0.6087 - val_categorical_true_positives: 14.0000 - val_get_f1: 0.6190 - val_precision: 0.4762 - val_recall: 1.0000\n",
            "Epoch 7/15\n",
            "7/7 [==============================] - 1s 86ms/step - loss: 2.6184 - accuracy: 0.7358 - categorical_true_positives: 39.0000 - get_f1: 0.6822 - precision: 0.5429 - recall: 1.0000 - val_loss: 2.1774 - val_accuracy: 0.6087 - val_categorical_true_positives: 14.0000 - val_get_f1: 0.6190 - val_precision: 0.4762 - val_recall: 1.0000\n",
            "Epoch 8/15\n",
            "7/7 [==============================] - 1s 86ms/step - loss: 1.5281 - accuracy: 0.8491 - categorical_true_positives: 45.0000 - get_f1: 0.6895 - precision: 0.5536 - recall: 1.0000 - val_loss: 2.1869 - val_accuracy: 0.6087 - val_categorical_true_positives: 14.0000 - val_get_f1: 0.6190 - val_precision: 0.4762 - val_recall: 1.0000\n",
            "Epoch 9/15\n",
            "7/7 [==============================] - 1s 93ms/step - loss: 3.4056 - accuracy: 0.6981 - categorical_true_positives: 37.0000 - get_f1: 0.6453 - precision: 0.5214 - recall: 1.0000 - val_loss: 1.8923 - val_accuracy: 0.7391 - val_categorical_true_positives: 17.0000 - val_get_f1: 0.6190 - val_precision: 0.4762 - val_recall: 1.0000\n",
            "Epoch 10/15\n",
            "7/7 [==============================] - 1s 94ms/step - loss: 1.7299 - accuracy: 0.6981 - categorical_true_positives: 37.0000 - get_f1: 0.6694 - precision: 0.5429 - recall: 1.0000 - val_loss: 1.0698 - val_accuracy: 0.7826 - val_categorical_true_positives: 18.0000 - val_get_f1: 0.6190 - val_precision: 0.4762 - val_recall: 1.0000\n",
            "Epoch 11/15\n",
            "7/7 [==============================] - 1s 92ms/step - loss: 1.4915 - accuracy: 0.7736 - categorical_true_positives: 41.0000 - get_f1: 0.6776 - precision: 0.5214 - recall: 1.0000 - val_loss: 1.6196 - val_accuracy: 0.6087 - val_categorical_true_positives: 14.0000 - val_get_f1: 0.6190 - val_precision: 0.4762 - val_recall: 1.0000\n",
            "Epoch 12/15\n",
            "7/7 [==============================] - 1s 90ms/step - loss: 0.7612 - accuracy: 0.8113 - categorical_true_positives: 43.0000 - get_f1: 0.6870 - precision: 0.5429 - recall: 1.0000 - val_loss: 3.1476 - val_accuracy: 0.6087 - val_categorical_true_positives: 14.0000 - val_get_f1: 0.6190 - val_precision: 0.4762 - val_recall: 1.0000\n",
            "Epoch 13/15\n",
            "7/7 [==============================] - 1s 87ms/step - loss: 0.8403 - accuracy: 0.8302 - categorical_true_positives: 44.0000 - get_f1: 0.6831 - precision: 0.5321 - recall: 1.0000 - val_loss: 3.7321 - val_accuracy: 0.6522 - val_categorical_true_positives: 15.0000 - val_get_f1: 0.6190 - val_precision: 0.4762 - val_recall: 1.0000\n",
            "Epoch 14/15\n",
            "7/7 [==============================] - 1s 87ms/step - loss: 1.5241 - accuracy: 0.7736 - categorical_true_positives: 41.0000 - get_f1: 0.6650 - precision: 0.5321 - recall: 1.0000 - val_loss: 1.8358 - val_accuracy: 0.6957 - val_categorical_true_positives: 16.0000 - val_get_f1: 0.6190 - val_precision: 0.4762 - val_recall: 1.0000\n",
            "Epoch 15/15\n",
            "7/7 [==============================] - 1s 90ms/step - loss: 0.8282 - accuracy: 0.7925 - categorical_true_positives: 42.0000 - get_f1: 0.6650 - precision: 0.5214 - recall: 1.0000 - val_loss: 1.0935 - val_accuracy: 0.8696 - val_categorical_true_positives: 20.0000 - val_get_f1: 0.6190 - val_precision: 0.4762 - val_recall: 1.0000\n",
            "Model training finished\n",
            "Validation f1: 0.619048\n",
            "Validation accuracy: 0.869565\n",
            "Validation Precision: 0.47619\n",
            "Validation Recall: 1.0\n"
          ]
        }
      ],
      "source": [
        "history = run_experiment(\n",
        "    model=tabtransformer_model,\n",
        "    train_data_file=train_data_file,\n",
        "    test_data_file=test_data_file,\n",
        "    num_epochs=NUM_EPOCHS,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    weight_decay=WEIGHT_DECAY,\n",
        "    batch_size=BATCH_SIZE,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "# model.summary()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": " tabtransformer",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "b0ec3465f5306ad2571dd352c55f43c441d7efdf3cf80b3e3c9f62414cf668dd"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
